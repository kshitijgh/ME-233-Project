# Define hyperparameters
learning_rate = 0.0001
batch_size = 64
num_epochs = 2000        ################################## NOTE: Higher Number of Epochs Needed
regularization_term = 5e-3
Lp = 1

Epoch 2000/2000, Train Loss: 0.0072
Epoch 2000/2000, Train R2 Score: 0.8184
Test Loss MSE: 0.0069
R2 Score: 0.8509
Layer: input_layer.weight, Weights: tensor([[ 7.3227e-02, -2.4750e-05, -1.5124e-04, -8.1997e-01,  3.9127e-05,
          1.2402e-05, -8.7364e-07, -7.4651e-05,  3.0670e-06, -3.6959e-05,
          1.1483e-04,  1.1521e-05,  3.1605e-05,  4.9630e-04,  1.9516e-05,
         -6.4763e-06, -6.2304e-05,  2.4605e-06, -7.2223e-06, -1.2441e-05,
         -9.5280e-06, -7.5389e-06,  6.6949e-02, -1.2303e-05,  1.7582e-05,
         -6.1662e-06, -1.2777e-05,  4.4649e-06,  5.8211e-06, -1.0186e-05,
         -6.1504e-06, -2.5372e-05,  1.0756e-05,  3.0851e-06]])

Number of Terms = 3
##############################################################################################



# Define hyperparameters
learning_rate = 0.0001
batch_size = 64
num_epochs = 1000
regularization_term = 1e-3
Lp = 1

Epoch 1000/1000, Train Loss: 0.0020
Epoch 1000/1000, Train R2 Score: 0.9868
Test Loss MSE: 0.0020
R2 Score: 0.9896
Layer: input_layer.weight, Weights: tensor([[ 3.3302e-01,  3.2416e-05, -3.4940e-01, -9.6725e-01,  2.9752e-04,
          3.0754e-05,  4.2415e-04,  2.3802e-06,  1.6113e-05,  1.1450e-05,
          1.2274e-01,  2.1204e-05,  3.9199e-05,  2.7756e-03,  4.1039e-05,
          1.7063e-08, -9.6782e-02,  1.1957e-05,  1.3378e-05, -1.7394e-05,
         -1.1228e-05,  1.7659e-05,  2.2361e-03, -1.1175e-05, -1.4827e-05,
         -8.7174e-06, -1.9820e-07, -1.3058e-05,  2.4303e-05, -2.3360e-06,
         -1.0259e-05,  4.2400e-05, -6.7889e-06,  1.4036e-05]])

Number of Terms = 5
##############################################################################################




# Define hyperparameters
learning_rate = 0.0001
batch_size = 64
num_epochs = 1000
regularization_term = 5e-4
Lp = 1

Epoch 1000/1000, Train Loss: 0.0011
Epoch 1000/1000, Train R2 Score: 0.9957
Test Loss MSE: 0.0011
R2 Score: 0.9966
Layer: input_layer.weight, Weights: tensor([[ 3.8133e-01, -8.8558e-05, -3.8266e-01, -9.1528e-01,  3.6982e-04,
         -2.3896e-02,  3.0548e-04, -1.5269e-01,  1.6685e-05,  2.2290e-05,
          1.0710e-01,  9.2451e-05,  3.2433e-05,  2.1994e-04,  1.1692e-04,
          2.9927e-05, -1.1220e-01, -7.8492e-06,  2.1659e-05,  1.7977e-05,
          1.9894e-05, -1.3003e-05,  4.5725e-04,  1.4262e-05, -5.7013e-07,
         -1.1884e-05, -5.3918e-06, -1.4294e-06, -2.2410e-06,  9.5709e-07,
         -5.6834e-07,  5.2169e-05, -2.6843e-05,  8.7656e-06]])

Number of Terms = 7
##############################################################################################



# Define hyperparameters
learning_rate = 0.0001
batch_size = 64
num_epochs = 1000
regularization_term = 1e-4
Lp = 1

Epoch 1000/1000, Train Loss: 0.0003
Epoch 1000/1000, Train R2 Score: 0.9993
Test Loss MSE: 0.0003
R2 Score: 0.9995
Layer: input_layer.weight, Weights: tensor([[ 4.3527e-01,  6.4945e-06, -4.7028e-01, -5.4193e-01,  1.2872e-04,
         -2.4468e-01,  2.2988e-04, -4.1523e-01, -4.5488e-06, -8.2152e-06,
          1.1401e-01, -7.1694e-05, -1.1657e-04,  2.4524e-04, -6.2474e-05,
         -6.5979e-05, -2.6117e-01, -1.2280e-05, -7.9770e-06, -1.2372e-05,
         -2.9712e-05,  2.8574e-05,  2.0926e-04,  9.8609e-06,  7.3154e-06,
          2.6873e-06, -2.0044e-05,  1.5173e-05,  2.5468e-06, -2.1060e-05,
         -1.2842e-05, -1.0340e-04,  1.6756e-05, -1.1517e-05]])

Number of Terms = 7
##############################################################################################



# Define hyperparameters
learning_rate = 0.0001
batch_size = 64
num_epochs = 1000
regularization_term = 5e-5
Lp = 1


Epoch 1000/1000, Train Loss: 0.0001
Epoch 1000/1000, Train R2 Score: 0.9995
Test Loss MSE: 0.0001
R2 Score: 0.9996
Layer: input_layer.weight, Weights: tensor([[ 4.3527e-01, -1.7632e-03, -4.8575e-01, -3.9778e-01,  8.5009e-06,
         -2.5112e-01, -9.7560e-02, -4.4149e-01, -7.0082e-05, -1.0040e-04,
          2.7837e-01, -1.8023e-04, -2.3648e-04,  2.9880e-05, -7.0892e-05,
         -1.2042e-04, -4.0191e-01, -3.7316e-06, -1.8471e-04, -5.2419e-06,
         -1.9552e-05, -1.9579e-05, -4.1147e-02, -1.6357e-05, -1.5530e-05,
          1.5970e-05, -5.4740e-06,  1.4724e-05,  1.4335e-05,  4.3469e-07,
         -2.5505e-06, -7.7109e-03,  1.7538e-05, -8.7426e-06]])

Number of Terms = 9
##############################################################################################



# Define hyperparameters
learning_rate = 0.0001
batch_size = 64
num_epochs = 1000
regularization_term = 1e-5
Lp = 1



Epoch 1000/1000, Train Loss: 0.0000
Epoch 1000/1000, Train R2 Score: 0.9996
Test Loss MSE: 0.0000
R2 Score: 0.9996
Layer: input_layer.weight, Weights: tensor([[ 4.2973e-01,  1.6418e-02, -4.8231e-01, -2.6182e-01, -6.8098e-02,
         -1.7949e-01, -1.4112e-01, -5.2943e-01, -6.3459e-05, -2.6634e-01,
          4.4586e-01, -4.9779e-05,  1.1459e-01,  4.0324e-02,  6.7687e-02,
          2.6428e-05, -5.0040e-01,  2.1903e-04, -1.6150e-01, -4.2364e-05,
          2.2822e-05,  1.2161e-01, -1.5308e-01,  1.7477e-04,  2.8712e-06,
          2.9366e-05, -7.4874e-06,  3.1887e-06, -1.1573e-05, -1.9884e-05,
          6.9575e-02, -1.0133e-01, -2.2522e-06,  6.5052e-06]])


Number of Terms = 19
##############################################################################################



# Define hyperparameters
learning_rate = 0.0001
batch_size = 64
num_epochs = 1000
regularization_term = 5e-6
Lp = 1


Epoch 1000/1000, Train Loss: 0.0000
Epoch 1000/1000, Train R2 Score: 0.9997
Test Loss MSE: 0.0000
R2 Score: 0.9997
Layer: input_layer.weight, Weights: tensor([[ 4.2426e-01,  3.4413e-02, -4.8480e-01, -2.5559e-01, -6.7885e-02,
         -1.8418e-01, -1.4460e-01, -5.4769e-01,  3.3384e-05, -3.3685e-01,
          4.6445e-01,  1.3420e-05,  1.6881e-01,  5.3937e-02,  1.0305e-01,
         -3.0072e-02, -4.9844e-01, -1.6907e-02, -2.6117e-01,  8.2164e-05,
          7.0245e-05,  1.9610e-01, -1.7765e-01,  9.6489e-02, -3.9412e-05,
         -1.3747e-05, -2.1287e-07, -5.7842e-06,  2.9531e-05,  9.8159e-06,
          3.4828e-01, -1.3219e-01, -2.2429e-05, -1.8569e-05]])


Number of Terms = 22
##############################################################################################